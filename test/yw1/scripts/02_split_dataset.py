# scripts/02_split_dataset.py
import shutil
import random
from pathlib import Path
from tqdm import tqdm
import yaml

# --- Configuration ---
# Define the split ratios for train, validation, and test sets.
# The sum should be 1.0.
TRAIN_RATIO = 0.8
VAL_RATIO = 0.2
TEST_RATIO = 0.0

if TRAIN_RATIO + VAL_RATIO + TEST_RATIO != 1.0:
    raise ValueError("The sum of TRAIN, VAL, and TEST ratios must be 1.0.")

# --- Paths ---
# Assumes the script is run from the project root 'deeplearning-repo-1'
ROOT_DIR = Path(__file__).resolve().parents[2]
BASE_DATA_DIR = ROOT_DIR / "yw1" / "data"
PROCESSED_DATA_DIR = BASE_DATA_DIR / "processed"
SOURCE_ROOTS = [
    BASE_DATA_DIR / "from_datacenter",
    BASE_DATA_DIR / "from_labelling"
]
NAMES = [
    "MountainDew", "MonsterEnergy", "PocariSweat", "BananaKick",
    "PocaChip", "Ojingeojip", "Yukgaejang", "Buldak", "SesameRamen"
]

def split_dataset():
    """
    Main function to split the dataset into training, validation, and test sets.
    It takes the files from `processed/images` and `processed/labels` and distributes
    them into `train`, `val`, and `test` subdirectories within those folders.
    """
    print("Starting dataset splitting...")
    data_by_class = {name: [] for name in NAMES} # (image_path, label_path) íŠœí”Œ ì €ì¥    

    # --- 1. Get list of all images ---
    for root in SOURCE_ROOTS:
            for name in NAMES:
                if root == SOURCE_ROOTS[1]:
                    img_dir = root / name / "images" /"Train" 
                    lbl_dir = root / name / "labels"/ "Train"
                else:
                    img_dir = root / name / "images"
                    lbl_dir = root / name / "labels"

                if img_dir.exists():
                    images = list(img_dir.glob("*.jpg")) + list(img_dir.glob("*.png"))
                    # for img_path in images:
                    #     # ë™ì¼í•œ ì´ë¦„ì˜ txt íŒŒì¼ ê²½ë¡œ ê³„ì‚°
                    #     lbl_path = lbl_dir / (img_path.stem + ".txt")
                    #     data_by_class[name].append((img_path, lbl_path))

                    # --- ìˆ˜ì •ëœ ë°ì´í„° ìˆ˜ì§‘ ë¡œì§ ---
                    for img_path in images:
                        lbl_path = lbl_dir / (img_path.stem + ".txt")
                        
                        # ì¡°ê±´ 1: ë¼ë²¨ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ê°€?
                        # ì¡°ê±´ 2: íŒŒì¼ì´ ì¡´ì¬í•œë‹¤ë©´ ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ê°€? (íŒŒì¼ í¬ê¸° ì²´í¬)
                        if lbl_path.exists():
                            data_by_class[name].append((img_path, lbl_path))
                        else:
                            # ë¼ë²¨ì´ ì—†ëŠ” ì´ë¯¸ì§€ëŠ” skipí•˜ê±°ë‚˜ ë³„ë„ ë¡œê·¸ ì¶œë ¥
                            # print(f"Skipping empty image: {img_path.name}")
                            pass

    final_sets = {"train" : [], "val" : [], "test" : []}
        

    # --- 2. í´ë˜ìŠ¤ë³„ë¡œ ìˆœíšŒí•˜ë©° 8:1:1 ë¶„í•  ---
    random.seed(42)
    for name, pairs in data_by_class.items():
        if not pairs:
            print(f"âš ï¸ ê²½ê³ : {name} ì¹´í…Œê³ ë¦¬ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue
            
        random.shuffle(pairs) # í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì…”í”Œ
        
        total = len(pairs)
        train_end = int(total * TRAIN_RATIO)
        val_end = int(total * (TRAIN_RATIO + VAL_RATIO))
        
        final_sets["train"].extend(pairs[:train_end])
        final_sets["val"].extend(pairs[train_end:val_end])
        final_sets["test"].extend(pairs[val_end:])
        
        print(f"ğŸ“Š {name.ljust(15)}: ì´ {total}ê°œ -> Train:{train_end}, Val:{val_end-train_end}, Test:{total-val_end}")

    # --- 3. í´ë” ìƒì„± ë° ë³µì‚¬ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼) ---
    for set_name, data_list in final_sets.items():
        dest_img_dir = PROCESSED_DATA_DIR / "images" / set_name
        dest_lbl_dir = PROCESSED_DATA_DIR / "labels" / set_name
        
        if dest_img_dir.exists():
            shutil.rmtree(dest_img_dir)
        if dest_lbl_dir.exists():
            shutil.rmtree(dest_lbl_dir)
        dest_img_dir.mkdir(parents=True, exist_ok=True)
        dest_lbl_dir.mkdir(parents=True, exist_ok=True)

        for img_path, lbl_path in tqdm(data_list, desc=f"Copying {set_name}"):
            shutil.copy(img_path, dest_img_dir / img_path.name)
            dest_lbl_path = dest_lbl_dir / lbl_path.name
            if lbl_path.exists():
                shutil.copy(lbl_path, dest_lbl_path)
            else:
                dest_lbl_path.touch()

    print("\nâœ… ëª¨ë“  ë°ì´í„° ë¶„í•  ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    print(f"ìœ„ì¹˜: {PROCESSED_DATA_DIR}")





def generate_yaml(output_path, processed_dir, class_names):
    """
    YOLOv11 í•™ìŠµì„ ìœ„í•œ data.yaml íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    # ìœˆë„ìš° í™˜ê²½ì—ì„œë„ ê²½ë¡œ ì¸ì‹ì„ ëª…í™•íˆ í•˜ê¸° ìœ„í•´ POSIX ìŠ¤íƒ€ì¼(forward slash)ë¡œ ë³€í™˜
    data_config = {
        'path': str(processed_dir.absolute()), # ë°ì´í„°ì…‹ ìµœìƒìœ„ ê²½ë¡œ
        'train': 'images/train',               # path ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
        'val': 'images/val',
        'test': 'images/test',
        'names': {i: name for i, name in enumerate(class_names)} # {0: 'MountainDew', 1: ...}
    }

    # yaml íŒŒì¼ ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(data_config, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
    
    print(f"âœ… YAML íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_path}")

# --- ì‹¤í–‰ êµ¬ê°„ ---



if __name__ == "__main__":
    split_dataset()

    # yaml_save_path = PROCESSED_DATA_DIR / "data.yaml"
    # generate_yaml(yaml_save_path, PROCESSED_DATA_DIR, NAMES)
